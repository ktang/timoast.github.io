<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Tim Stuart</title>
 <link href="/atom.xml" rel="self"/>
 <link href="/"/>
 <updated>2016-02-23T17:35:52+08:00</updated>
 <id></id>
 <author>
   <name>Tim Stuart</name>
   <email>timstuart90@gmail.com</email>
 </author>

 
 <entry>
   <title>A paper a day</title>
   <link href="//2016/02/23/Papers/"/>
   <updated>2016-02-23T00:00:00+08:00</updated>
   <id>/2016/02/23/Papers</id>
   <content type="html">&lt;p&gt;In an effort to read more papers this year, I’m going to read a paper (or something paper-like) each day for the remainder of the year, and post each paper below as I go.&lt;/p&gt;

&lt;h3 id=&quot;feb-23&quot;&gt;Feb 23&lt;/h3&gt;

&lt;p&gt;Crisp PA, Ganguly D, Eichten SR, Borevitz JO, Pogson BJ. 2016. Reconsidering plant memory: Intersections between stress recovery, RNA turnover, and epigenetics. Science Advances 2: e1501340–e1501340. &lt;a href=&quot;http://advances.sciencemag.org/content/advances/2/2/e1501340.full&quot;&gt;doi:10.1126/sciadv.1501340&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Interesting or important points:&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;An as yet entirely unexplored possibility is the prospect that transcriptional memory might be underpinned by changes in mRNA stability rather than transcription.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I think certain RNA modifications have been shown to be induced by stress (at least in yeast), perhaps they play a role in modifying mRNA stability to facilitate stress memory.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Lorne Genome 2016 presentation</title>
   <link href="//2016/02/18/Lorne/"/>
   <updated>2016-02-18T00:00:00+08:00</updated>
   <id>/2016/02/18/Lorne</id>
   <content type="html">&lt;p&gt;This week I presented some recent work at the Lorne Genome conference. It was my first time at Lorne and it’s a fantastic meeting with lots of interesting talks. It was great to see so many people getting interested in transposon biology too!&lt;/p&gt;

&lt;p&gt;Here are a few links to the work I was presenting:&lt;/p&gt;

&lt;p&gt;TEPID TE presence/absence variant discovery software:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/ListerLab/TEPID&quot;&gt;https://github.com/ListerLab/TEPID&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Poster:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://dx.doi.org/10.6084/m9.figshare.2082757.v1&quot;&gt;https://dx.doi.org/10.6084/m9.figshare.2082757.v1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Manuscript:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://biorxiv.org/content/early/2016/02/11/039511&quot;&gt;http://dx.doi.org/10.1101/039511&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Extract reads from bam file by read name</title>
   <link href="//2015/10/12/ExtractReads/"/>
   <updated>2015-10-12T00:00:00+08:00</updated>
   <id>/2015/10/12/ExtractReads</id>
   <content type="html">&lt;p&gt;While there are very fast and easy ways to extract reads from a bam file according to mapping location, extracting reads by read name is more difficult.&lt;/p&gt;

&lt;p&gt;Simple methods, like using &lt;code&gt;grep&lt;/code&gt;, are incredibly slow if you want to look for more than a few reads.&lt;/p&gt;

&lt;p&gt;Luckily, &lt;code&gt;pysam&lt;/code&gt; allows you to index a bam file by read name (using &lt;code&gt;pysam.IndexedReads(AlignmentFile)&lt;/code&gt;) while keeping the sort order. However, the index is stored in memory so this can use a lot of RAM (my tests with a 5.7 GB bam file used about 9 GB RAM).&lt;/p&gt;

&lt;p&gt;I wrote a small python script (below) that uses pysam to extract reads by read name from a bam file.&lt;/p&gt;

&lt;p&gt;Extracting 10 reads from a 5.7 GB bam file, just using &lt;code&gt;grep&lt;/code&gt; is slightly faster than the python script:&lt;/p&gt;

&lt;p&gt;```shell
timstuart Altai-5$  time samtools view Altai-5_filtered.bam | grep -f reads.txt &amp;gt; extracted&lt;/p&gt;

&lt;p&gt;real    2m10.088s
user    2m23.107s
sys 0m35.470s&lt;/p&gt;

&lt;p&gt;timstuart Altai-5$  time python extract_reads.py -b Altai-5_filtered.bam -n reads.txt -o python_extracted.bam&lt;/p&gt;

&lt;p&gt;real    3m36.444s
user    3m14.744s
sys 0m18.867s&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;p&gt;But extracting 200 reads using python is fast, while using &lt;code&gt;grep&lt;/code&gt; ran for over an hour before I stopped the process.&lt;/p&gt;

&lt;p&gt;```shell
timstuart Altai-5$  time python extract_reads.py -b Altai-5_filtered.bam -n reads.txt -o python_extracted.bam&lt;/p&gt;

&lt;p&gt;real    3m30.015s
user    3m12.738s
sys 0m15.055s
```&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/timoast/2264a79f93b3f1cb3aac.js&quot;&gt;&lt;/script&gt;

</content>
 </entry>
 
 <entry>
   <title>Estimation of insert size from sam / bam files</title>
   <link href="//2014/11/04/SizeEstimation/"/>
   <updated>2014-11-04T00:00:00+08:00</updated>
   <id>/2014/11/04/SizeEstimation</id>
   <content type="html">&lt;p&gt;I’ve been looking for a simple way to estimate paired-end insert sizes from mapped data. There are a few more complicated tools available (I think picard will do it), as well as simple scripts different people have written that don’t exclude outliers. However outliers due to discordant read alignments will hugely change the mean, so need to be removed.&lt;/p&gt;

&lt;p&gt;As I couldn’t find anything that was a nice middle ground, I ended up writing my own simple python script that will take input from stdin and exclude outliers more that two standard deviations from the median, and print the mean and standard deviation to stdout.&lt;/p&gt;

&lt;p&gt;It can be used to estimate the insert size from sam or bam files:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ head -10000 mapped.sam | python mean_size.py
220 35
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
$ samtools view mapped.bam | head -10000 | python mean_size.py
220 35
&lt;/code&gt;&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/timoast/af73c0e9fac00187ee49.js&quot;&gt;&lt;/script&gt;

</content>
 </entry>
 
 <entry>
   <title>MethyC-Seq Analysis Notes</title>
   <link href="//2014/10/29/MethyC-Analysis/"/>
   <updated>2014-10-29T00:00:00+08:00</updated>
   <id>/2014/10/29/MethyC-Analysis</id>
   <content type="html">&lt;h2 id=&quot;preliminary-steps&quot;&gt;Preliminary steps&lt;/h2&gt;

&lt;h3 id=&quot;bcl-conversion&quot;&gt;Bcl-conversion&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Enter a screen&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;code&gt;bash
screen -S bcl-conversion
&lt;/code&gt;
2. Navigate to directory with run (eg. 130909_SNL119_0105_AC2GYKACXX)
3. Check sample sheet configured correctly. If only one adapter in lane, remove adapter sequence from sample sheet.
4. Run the following (modified with correct run name, sample sheet etc). Can change final value to change number of reads in files:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
/usr/local/packages/CASAVA_v1.8.2/bcl2fastq/build/bin/configureBclToFastq.pl --input-dir /dd_rundata/hiseq/Runs/130909_SNL119_0105_AC2GYKACXX/Data/Intensities/BaseCalls/ --sample-sheet /dd_rundata/hiseq/Runs/130909_SNL119_0105_AC2GYKACXX/SampleSheet.csv --fastq-cluster-count 50000000
&lt;/code&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Navigate to newly created Unaligned directory (under top run directory) and enter:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;code&gt;bash
nohup make -j 12
&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;moving-and-renaming-files&quot;&gt;Moving and renaming files&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Copy run files from run directory to working directory:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;code&gt;bash
cp Project_E_grandis ~/working_data
&lt;/code&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Rename fastq files to &lt;code&gt;s_1_sequence.txt&lt;/code&gt;, &lt;code&gt;s_2_sequence.txt&lt;/code&gt; etc.&lt;/li&gt;
  &lt;li&gt;Store sequence files a separate directory, eg. sequences. If you have data from the same library but multiple runs, store in separate directories.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;mapping&quot;&gt;Mapping&lt;/h2&gt;

&lt;p&gt;Can do multiple samples at a time&lt;/p&gt;

&lt;p&gt;Use &lt;code&gt;map.php&lt;/code&gt; (0 mismatches, or use &lt;code&gt;map_1mm.php&lt;/code&gt; or &lt;code&gt;map_2mm.php&lt;/code&gt; for 1 or 2 mismatches) to map all the reads to the genome (follow instructions):&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
php /home/lister/working_data/php/methpipe_se/map.php | tee -a log.txt
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;For PE data&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
php /home/lister/working_data/php/methpipe_pe/map.php | tee -a log.txt
&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;post-map&quot;&gt;Post-map&lt;/h2&gt;

&lt;p&gt;Do one sample at a time. This step will generate all the tables in mySQL (used for AnnoJ and DMR script).&lt;/p&gt;

&lt;p&gt;This script can take mapped read runs, merge sets, convert to .slam format, sort reads,  collapse, trim, split, stack, hammer, import reads, stacks and mC to MYSQL.&lt;/p&gt;

&lt;p&gt;Start with a mapped dir containing the subdir that contain the &lt;code&gt;\*_final&lt;/code&gt; mapped files.&lt;/p&gt;

&lt;p&gt;Navigate to directory above mapped run data and start a screen:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
screen -S postmap
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Start the postmap script as follows:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
php /home/lister/working_data/php/methpipe_se/post_map.php | tee -a log.txt
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;You will see the following prompts:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
  -Do you want to perform stage 1 (merge mapped runs, convert to .slam, sort, collapse, trim reads) (y/n): y
  -Do you want to perform stage 2 (import mapped reads into MYSQL) (y/n): y
  -Do you want to perform stage 3 (stack and hammer) (y/n): y
  -Do you want to perform stage 4 (import stacks into MYSQL) (y/n): y
  -Do you want to perform stage 5 (import mC&#39;s into MYSQL) (y/n): y
  -Do you want to perform stage 6 (correct mammalian mCH for genotype) (y/n): n
  -Do you want to perform stage 7 (make and import allC tables) (y/n): y
  -Do you want to perform stage 8 (identify partially methylated domains) (y/n): n
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Enter the path to mapped folder when prompted.&lt;/p&gt;

&lt;p&gt;Shows a summary of all options, then:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
- Based on these settings, do you want to proceed (y/n): y
- Number of libraries that make up the sample: 1
- Enter run folder names in library 1 (space delim): sequences  &amp;lt;-- name of sample folder
&lt;/code&gt;&lt;/p&gt;

&lt;h1 id=&quot;methylpy-dmr-finder&quot;&gt;Methylpy DMR finder&lt;/h1&gt;

&lt;p&gt;Add the following to your &lt;code&gt;.bashrc&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
alias methylenv=&#39;source /usr/local/virtualenv/methylenv/bin/activate; export PYTHONPATH=/usr/local/packages/methylpy:/usr/local/packages/methylpy/methylpy&#39;
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;All Methylpy steps must be done in using the methylenv. To exit the methylenv, type &lt;code&gt;deactivate&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If you don’t have the package already it can be cloned from bitbucket:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
git clone git@bitbucket.org:schultzmattd/methylpy.git
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;To update, from the directory created by the clone:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
git pull
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;To test methylpy:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
python methylpy_test.py
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;There are 3 steps to the DMR finding algorithm:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Perform a root mean square test (you can think of it like a chisquare test) on each site across all samples. P-values are simulated (i.e., randomize the data a bunch of times and see if you get a significant result), which adjusts for multiple testing.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Calculate threshold p-value for a desired FDR.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Aggregate any significant sites within X bp and showing changes in the same direction (e.g, sample A is methylated and sample B is unmethylated) into a window.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;generation-of-allc-files&quot;&gt;Generation of allC files&lt;/h2&gt;

&lt;p&gt;Edit the allC generating script: &lt;code&gt;create_allc_file_template_hs.py&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Sample base names:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;python
[&quot;sample_1_name&quot;, &quot;sample_2_name&quot;]
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Database names:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;python
[&quot;database_name&quot;,&quot;database_name&quot;]
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;MySQL server:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;python
[&quot;localhost&quot;,&quot;localhost&quot;]
&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;run-allc-generating-script&quot;&gt;Run allC generating script&lt;/h3&gt;
&lt;p&gt;Move to folder called “allC”:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
$ methylenv
(methylenv) $ python create_allc_file_egrandis.py
&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;dmrfind&quot;&gt;DMRfind&lt;/h2&gt;

&lt;p&gt;Run all samples at the same time within an experiment.&lt;/p&gt;

&lt;p&gt;Edit the python script named &lt;code&gt;DMR_find.py&lt;/code&gt; with your sample names and parameters.&lt;/p&gt;

&lt;p&gt;Run script, in a folder named “DMR”:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
python dmr_find.py &amp;gt; dmr_find_ouput.txt
&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;histogram-correction&quot;&gt;Histogram correction&lt;/h2&gt;

&lt;p&gt;Modify &lt;code&gt;histogram_correction.py&lt;/code&gt; script with name of &lt;code&gt;_rms_results.tsv&lt;/code&gt; file from allC step.&lt;/p&gt;

&lt;p&gt;Run script:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
python histogram_correction.py &amp;gt;&amp;gt; histogram_correction_output.txt
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Use the p-value determined by histogram correction for the collapse step.&lt;/p&gt;

&lt;h2 id=&quot;collapse&quot;&gt;Collapse&lt;/h2&gt;

&lt;p&gt;Edit the collapse.py script with your sample names and parameters. This may need to be changed and run several times to find the right parameters.
Run the script on the &lt;code&gt;_rms_results.tsv&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
python collapse.py
&lt;/code&gt;
 &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Useful python code</title>
   <link href="//2014/08/25/UsefulPython/"/>
   <updated>2014-08-25T00:00:00+08:00</updated>
   <id>/2014/08/25/UsefulPython</id>
   <content type="html">&lt;h3 id=&quot;take-command-line-arguments&quot;&gt;Take command-line arguments&lt;/h3&gt;

&lt;p&gt;```python
from argparse import ArgumentParser&lt;/p&gt;

&lt;p&gt;version = pkg_resources.require(“program”)[0].version
parser = ArgumentParser(description=’program description’)
group = parser.add_mutually_exclusive_group()
parser.add_argument(‘–version’, action=’version’, version=’%(prog)s ‘+str(version))
parser.add_argument(‘–option’, help=’option description’, required=False, default=False, action=’store_true’)
parser.add_argument(‘-n’, ‘–name’, help=’sample name’, required=True)
```&lt;/p&gt;

&lt;h2 id=&quot;lists&quot;&gt;Lists&lt;/h2&gt;

&lt;h3 id=&quot;list-comprehension&quot;&gt;List comprehension&lt;/h3&gt;

&lt;p&gt;If ever declaring and empty list then using &lt;code&gt;.append&lt;/code&gt;, can be done with list comprehension.&lt;/p&gt;

&lt;p&gt;```python
list3 = []
for x in list2:
    if x[0] in list1:
        list3.append(x)&lt;/p&gt;

&lt;p&gt;list3 = [x for x in list2 if x[0] in list1]
```&lt;/p&gt;

&lt;p&gt;Can also use this to read from a file in one line, eg to read column 4 in a file into a list in python:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;python
all_TEs = [line.rsplit()[4] for line in open(&quot;TAIR9_TE.bed&quot;, &quot;r&quot;)]
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Or read all lines into a list:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;python
all_lines = [line.strip(&quot;\n&quot;) for line in open(&quot;filename&quot;, &quot;r&quot;)]
&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;join-items-in-list&quot;&gt;Join items in list&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;python
outfile.write(&#39;{l}\n&#39;.format(l=&#39;\t&#39;.join(list)))
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If list items are not strings&lt;/p&gt;

&lt;p&gt;&lt;code&gt;python
outfile.write(&#39;{l}\n&#39;.format(l=map(str, list)))
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Or using list comprehension&lt;/p&gt;

&lt;p&gt;&lt;code&gt;python
stringVersion = [str(x) for x in inputList]
&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;printing&quot;&gt;Printing&lt;/h2&gt;

&lt;h3 id=&quot;print-in-colour&quot;&gt;Print in colour&lt;/h3&gt;

&lt;p&gt;```python
class colour:
    PURPLE = ‘\033[95m’
    CYAN = ‘\033[96m’
    DARKCYAN = ‘\033[36m’
    BLUE = ‘\033[94m’
    GREEN = ‘\033[92m’
    YELLOW = ‘\033[93m’
    RED = ‘\033[91m’
    BOLD = ‘\033[1m’
    UNDERLINE = ‘\033[4m’
    END = ‘\033[0m’&lt;/p&gt;

&lt;p&gt;print colour.CYAN + ‘This is blue’ + colour.END
```&lt;/p&gt;

&lt;h3 id=&quot;format-text-wrapping-correctly-in-terminal&quot;&gt;Format text wrapping correctly in terminal&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;python
import textwrap
print textwrap.dedent(&#39;print message&#39;)
&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;strings&quot;&gt;Strings&lt;/h2&gt;

&lt;h3 id=&quot;split-strings-by-multiple-delimiters&quot;&gt;Split strings by multiple delimiters&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;python
accession_name = f.replace(&#39;calls_&#39;, &#39;.&#39;).split(&#39;.&#39;)
&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;file-reading--writing&quot;&gt;File reading / writing&lt;/h2&gt;

&lt;h3 id=&quot;read-lines-in-tsvcsv-file&quot;&gt;Read lines in tsv/csv file&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;python
with open(&#39;merged_dmrs.bed&#39;, &#39;r&#39;) as f:
    for line in f:
        line = line.rsplit() # each item in line is now part of a list
&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;find-if-coordinates-overlap&quot;&gt;Find if coordinates overlap&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;python
def overlap(start1, stop1, start2, stop2):
    &quot;&quot;&quot;returns True if sets of coordinates overlap. Assumes coordinates are on same chromosome&quot;&quot;&quot;
    for y in xrange(start2, stop2+1):
        if start1 &amp;lt;= y &amp;lt;= stop1:
            return True
        else:
            pass
&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;functions-for-mapping-ngs-reads&quot;&gt;Functions for mapping NGS reads&lt;/h2&gt;

&lt;p&gt;```python
import os
from subprocess import call&lt;/p&gt;

&lt;p&gt;def peMap(proc):
    “””
    paired-end reads
    “””
    for filename in os.listdir(‘.’):
        if filename.endswith(“&lt;em&gt;1.fastq”):
            sam = filename.split(‘&lt;/em&gt;’)
            sam = sam[0]
            call([“bowtie2”, “-p{x}”.format(x=proc), “-q”, “-x /dd_stage/userdata/lister/data/genomes/bowtie2_indexes/tair9”,
                  “-1 {f}”.format(f=sam + ‘_1.fastq’), “-2 {f}”.format(f=sam + ‘_2.fastq’), “-S {s}”.format(s=sam + ‘.sam’)])
        else:
            pass&lt;/p&gt;

&lt;p&gt;def seMap(proc):
    “””
    single-end reads
    “””
    for filename in os.listdir(‘.’):
        if filename.endswith(“.fastq”):
            sam = filename.split(‘.’)
            sam = sam[0]
            call([“bowtie2”, “-p{x}”.format(x=proc), “-q”, “-x /dd_stage/userdata/lister/data/genomes/bowtie2_indexes/tair9”,
                  “-U {f}”.format(f=filename), “-S {s}”.format(s=sam + ‘.sam’)])
        else:
            pass
```&lt;/p&gt;

&lt;h2 id=&quot;split-sra-files-into-fastq&quot;&gt;Split sra files into fastq&lt;/h2&gt;

&lt;p&gt;```python
import os
from subprocess import call&lt;/p&gt;

&lt;p&gt;def fastqSplit():
    for filename in os.listdir(‘.’):
        if filename.endswith(‘.sra’):
            print ‘processing {n}’.format(n=filename)
            call([‘fastq-dump’, ‘–split-3’, ‘-v’, filename])
        else:
            pass
```&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Useful Linux/Unix commands</title>
   <link href="//2014/08/24/UsefulCommands/"/>
   <updated>2014-08-24T00:00:00+08:00</updated>
   <id>/2014/08/24/UsefulCommands</id>
   <content type="html">&lt;h2 id=&quot;go-back-to-last-directory&quot;&gt;go back to last directory&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;
cd -
&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;do-something-in-a-range&quot;&gt;Do something in a range&lt;/h2&gt;
&lt;p&gt;Move files&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
for i in $(seq 10); do mv chr$i ../genomes/; done
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
for directory in ./*; do
    if [ -d &quot;$directory&quot; ]; then
        cd $directory
        for myfile in $(ls -d *.sra);do
            mv $myfile /home/tstuart/working_data/1000genomes/$myfile
        done
        cd ..
    fi
done
&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;take-command-line-arguments&quot;&gt;Take command-line arguments&lt;/h2&gt;

&lt;p&gt;Required flags are followed by &lt;code&gt;:&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;```bash
index=  proc=  path=&lt;/p&gt;

&lt;p&gt;while getopts x:pA opt; do
  case $opt in
  x)
      index=$OPTARG
      ;;
  p)
      proc=$OPTARG
      ;;
  A)
      path=${OPTARG%/}
      ;;
  esac
done
shift $((OPTIND - 1))
```&lt;/p&gt;

&lt;h2 id=&quot;strip-trailing-slash-from-string&quot;&gt;Strip trailing slash from string&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;bash
path=${OPTARG%/}
&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;gnu-screen&quot;&gt;GNU Screen&lt;/h2&gt;

&lt;p&gt;Start screen: &lt;code&gt;screen -S [screen name]&lt;/code&gt;&lt;br /&gt;
List screens: &lt;code&gt;screen -list&lt;/code&gt;&lt;br /&gt;
Detach: &lt;code&gt;screen -d&lt;/code&gt;&lt;br /&gt;
Attach: &lt;code&gt;screen -r&lt;/code&gt;&lt;br /&gt;
Close screen: &lt;code&gt;ctr-a-d&lt;/code&gt;&lt;br /&gt;
Kill screen: &lt;code&gt;ctr-a :quit&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;human-readable-path&quot;&gt;Human-readable path&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;bash
echo -e ${PATH//:/&#39;\n&#39;}
&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;searching&quot;&gt;Searching&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;bash
grep &#39;text&#39; filename
grep &#39;text&#39; file1 file2 file3
grep &#39;text1 text2&#39; filename
grep --color &#39;text&#39; filename
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Search all files in a directory, show output in less&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
grep -r &#39;text&#39; /home/usr/ | less
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Search for multiple strings&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
egrep &#39;(AT4G40030)|(AT4G40040)|(AT5G10980)&#39; * &amp;gt; h3.3
&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;pipe-output-into-file&quot;&gt;Pipe output into file&lt;/h2&gt;
&lt;p&gt;#### Overwrite contents of file&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
grep &quot;text&quot; filename.txt &amp;gt; output.txt
&lt;/code&gt;&lt;/p&gt;

&lt;h4 id=&quot;append-to-file&quot;&gt;Append to file&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;bash
grep &quot;text&quot; filename.txt &amp;gt;&amp;gt; output.txt
&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;counting&quot;&gt;Counting&lt;/h2&gt;
&lt;p&gt;Count lines in chr1 file&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
wc -l chr1
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Count characters in chr1 file&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
wc -c chr1
&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;list&quot;&gt;List&lt;/h2&gt;

&lt;p&gt;List and sort&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
ls -ls
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;List and sort by size&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
ls -ls -S
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Count number of directories in current directory&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
ls -l | grep ^d | wc -l
&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;sorting&quot;&gt;Sorting&lt;/h2&gt;

&lt;p&gt;Sort file by numerical order of first column, save as sorted_list.txt&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
sort -nk 1 list.txt &amp;gt; sorted_list.txt
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Sort file by order of first column, then numerical order of second column, save as sorted_list.txt&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
sort -k1,1 -nk2,2 list.txt &amp;gt; sorted_list.txt
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Sort descending&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
sort -rn -k3 file.txt &amp;gt; sorted.txt
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;overwrite&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
sort -rn -k3 -o file.txt file.txt
&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;rows&quot;&gt;Rows&lt;/h2&gt;
&lt;p&gt;Delete first line&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
sed 1d file.txt &amp;gt; headerless_file.txt
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Delete lines 1-3 inclusive&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
sed 1,3d file.txt &amp;gt; newfile.txt
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Delete lines containing string&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
sed &#39;/ATC/d&#39; file.txt &amp;gt; file_mod.txt
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Overwrite file&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
sed -i.bak 1d file.txt
&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;columns&quot;&gt;Columns&lt;/h2&gt;
&lt;p&gt;Write columns to new file. Can also reorder columns.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
awk &#39;BEGIN {FS=OFS=&quot;\t&quot;} {print $2,$4,$5,$6,$7,$8,$1,$3}&#39; file.txt &amp;gt; outfile.txt
awk &#39;BEGIN {FS=OFS=&quot;\t&quot;} {print $7,$1,$8,$2,$3,$4,$5,$6}&#39; cmt2_targets.tsv &amp;gt; cmt2_targets_ordered.tsv
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Add column&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
awk &#39;{print $0, &quot;cmt2&quot;}&#39; cmt2_targets_ordered.tsv &amp;gt; newfile.tsv
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Remove first column from all files&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
for myfile in $(ls);do
    awk &#39;BEGIN {FS=OFS=&quot;\t&quot;} {$1=&quot;&quot;;sub(&quot;\t&quot;,&quot;&quot;)}1&#39; $myfile &amp;gt; mod_$myfile
done
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Search only in one column&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
awk &#39;{if ($1 == 1) print $2}&#39; p1 &amp;gt; chr1_p1
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Add string prefix to a column, eg. add ‘chr’ to the first column&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
awk &#39;BEGIN {FS=OFS=&quot;\t&quot;} {$1=&quot;chr&quot;$1; print $0}&#39; input_file.txt &amp;gt; output_file.txt
&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;joining-files&quot;&gt;Joining files&lt;/h2&gt;

&lt;p&gt;Join files with each starting on a new line&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
for filename in *.fa; do
    cat &quot;${filename}&quot;
    echo
done &amp;gt; output.fa
&lt;/code&gt;
Merge files of the same format&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
cat p1_* | sort -nk1,1 -k2,2 | uniq &amp;gt; p1
&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;comparing-files&quot;&gt;Comparing Files&lt;/h2&gt;

&lt;p&gt;Files should first be sorted&lt;/p&gt;

&lt;p&gt;Find lines that are common or different between two files&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
comm -i file1 file2 &amp;gt; output.txt
&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;-i&lt;/code&gt; is case-insensitive&lt;/li&gt;
  &lt;li&gt;outputs three columns. First is lines only in file1, second is lines only in file2, third is lines common in both.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;compressing-files&quot;&gt;Compressing files&lt;/h2&gt;

&lt;p&gt;Compress recursively and store all files in a single compressed folder&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
tar cvfz slam.tgz slam/
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Compress recursively&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
gzip -r directory/
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Decompress files&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
tar -x -f file.tar
tar -x -z -f file.tgz
gunzip file.gz
&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;download-data-from-sra&quot;&gt;Download data from SRA&lt;/h2&gt;

&lt;h3 id=&quot;using-wget&quot;&gt;Using wget&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;
wget -r --no-parent --reject &quot;index.html*&quot; ftp://ftp-trace.ncbi.nih.gov/sra/sra-instant/reads/ByStudy/sra/SRP/SRP005/SRP005399/
&lt;/code&gt;
Alternatively, you can download a list of accessions from the SRA website and use that file to call &lt;code&gt;wget&lt;/code&gt; for all the accessions.&lt;/p&gt;

&lt;p&gt;```python
from subprocess import call&lt;/p&gt;

&lt;p&gt;with open(‘SraAccList.txt’, ‘r’) as accessions:
    for row in accessions:
        acc = row.strip(‘\n’)
        prefix = acc[:6]
        print “Downloading {a}”.format(a=acc)
        call([“wget”,
              “-r”,
              “–no-parent”,
              “ftp://ftp-trace.ncbi.nih.gov/sra/sra-instant/reads/ByRun/sra/SRR/{pre}/{acc}/*“.format(pre=prefix, acc=acc)])&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;h4 id=&quot;split-into-fastq-files&quot;&gt;Split into fastq files&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;bash
fastq-dump --split-3 ./SRP005399/*
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;--split-3&lt;/code&gt; causes PE files to be split into &lt;code&gt;*_1.fastq&lt;/code&gt; and &lt;code&gt;*_2.fastq&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;bash loop&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
for directory in ./*; do
    if [ -d &quot;$directory&quot; ]; then
        cd $directory
        for myfile in $(ls -d *.sra);do
            fastq-dump --split-3 -v $myfile
        done
        cd ..
    fi
done
&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Python function&lt;/p&gt;

&lt;p&gt;```python
import os
from subprocess import call&lt;/p&gt;

&lt;p&gt;def fastqSplit():
    for filename in os.listdir(‘.’):
        if filename.endswith(‘.sra’):
            print ‘processing {n}’.format(n=filename)
            call([‘fastq-dump’, ‘–split-3’, ‘-v’, filename])
        else:
            pass
```&lt;/p&gt;

&lt;h3 id=&quot;using-sra-toolkit&quot;&gt;Using SRA toolkit&lt;/h3&gt;

&lt;p&gt;SRA toolkit doesn’t seem to work well for downloading bulk data. &lt;code&gt;wget&lt;/code&gt; is a much better option as it allows download of whole folders, gives more descriptive output as it goes. SRA often fails and sometimes gives no error. Download with &lt;code&gt;wget&lt;/code&gt; and use &lt;code&gt;fastq-split&lt;/code&gt; to get fastq files.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;bash
fastq-dump SRR534224 &amp;amp;
&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;rna-seq&quot;&gt;RNA-seq&lt;/h2&gt;

&lt;h3 id=&quot;mapping&quot;&gt;Mapping&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;
tophat -p8 -G /home/lister/working_data/data/genomes/annotations/tair10/TAIR10_gene_TE_illumina.gtf --transcriptome-index=transcript_index/tair10 /home/lister/working_data/data/genomes/bowtie2_indexes/tair9 SRR501604.fastq,SRR501605.fastq
&lt;/code&gt;
* &lt;code&gt;--transcriptome-index=transcript_index/tair10&lt;/code&gt; creates transcriptome index file saved in transcript_index/ with name &lt;code&gt;tair10.*&lt;/code&gt;. This can be reused if mapping to the same transcriptome (ie. gff file)&lt;br /&gt;
* &lt;code&gt;-p8&lt;/code&gt; uses 8 cores&lt;br /&gt;
* &lt;code&gt;-G&lt;/code&gt; specifies gff file. Optional&lt;br /&gt;
* If using PE reads check manual&lt;/p&gt;

&lt;h2 id=&quot;bedtools&quot;&gt;Bedtools&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://quinlanlab.org/tutorials/cshl2014/bedtools.html&quot;&gt;Good tutorial from the Quinlan lab&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Need to order files so that chromosome, start, stop and first three columns (BED format). Also need to remove header. Can do these steps with &lt;code&gt;awk&lt;/code&gt; and &lt;code&gt;sed 1d&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;merge&quot;&gt;Merge&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;bash
bedtools merge -i dmrs_ddc.tsv -d 100 &amp;gt; dmrs_ddc_merged.tsv
&lt;/code&gt;
* &lt;code&gt;-d&lt;/code&gt; is distance between coordinates that will still be merged&lt;br /&gt;
* &lt;code&gt;-i&lt;/code&gt; is input file&lt;/p&gt;

&lt;h3 id=&quot;intersect&quot;&gt;Intersect&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;bash
bedtools intersect -a tair10_tes.txt -b dmrs_ddc_merged.tsv -wa -f 0.50 &amp;gt; ddc_targets.tsv
&lt;/code&gt;
* &lt;code&gt;-a&lt;/code&gt; is file a&lt;br /&gt;
* &lt;code&gt;-b&lt;/code&gt; is file b&lt;br /&gt;
* &lt;code&gt;-wa&lt;/code&gt; is write a&lt;br /&gt;
* &lt;code&gt;-f&lt;/code&gt; is minimum overlap percentage&lt;/p&gt;

&lt;h2 id=&quot;bowtie&quot;&gt;Bowtie&lt;/h2&gt;

&lt;h3 id=&quot;mapping-pe-data&quot;&gt;Mapping PE data&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;
bowtie2 -p8 --local --fr -q -R 5 -N 1 -x [path/to/bowtie2/index] -X 1000 -1 [mate1.fq] -2 [mate2.fq] -S [alignment.sam] --un-conc ./discordant/
&lt;/code&gt;
* &lt;code&gt;-p&lt;/code&gt; is number of processors&lt;br /&gt;
* &lt;code&gt;--local&lt;/code&gt; allows soft-clipping of reads to improve alignment&lt;br /&gt;
* &lt;code&gt;-q&lt;/code&gt; specifies that reads are in fastq format&lt;br /&gt;
* &lt;code&gt;-x&lt;/code&gt; gives path to index&lt;br /&gt;
* &lt;code&gt;-S&lt;/code&gt; gives name of output file&lt;br /&gt;
* &lt;code&gt;-R&lt;/code&gt; is number of times bowtie will try to ‘re-seed’ repetitive seeds. Default 2&lt;br /&gt;
* &lt;code&gt;-N&lt;/code&gt; is number of mismatches allowed in seed. 0 or 1.&lt;br /&gt;
* &lt;code&gt;--no-mixed&lt;/code&gt; tells bowtie to find alignments only when both pairs can be aligned.&lt;br /&gt;
* &lt;code&gt;--fr&lt;/code&gt; means mate pairs are ordered in forward then reverse orientation. Can do &lt;code&gt;--ff&lt;/code&gt;, &lt;code&gt;--rf&lt;/code&gt;.&lt;br /&gt;
* &lt;code&gt;-X&lt;/code&gt; specifies maximum insert size. Default 500.&lt;br /&gt;
* &lt;code&gt;-I&lt;/code&gt; specifies minimum insert size. Default 0 (no minimum).&lt;br /&gt;
* &lt;code&gt;--un-conc&lt;/code&gt; specifies path and file to write discordant alignments to. Note that these are just the &lt;code&gt;fastq&lt;/code&gt; reads, not alignments.&lt;/p&gt;

&lt;h2 id=&quot;samtools&quot;&gt;Samtools&lt;/h2&gt;

&lt;p&gt;samtools can read from a stream, so can pipe output in from other tools (eg bowtie to get &lt;code&gt;.bam&lt;/code&gt; output) or other samtools commands.&lt;/p&gt;

&lt;h3 id=&quot;convert-from-sam-to-bam&quot;&gt;Convert from &lt;code&gt;sam&lt;/code&gt; to &lt;code&gt;bam&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;
samtools view -bS file.sam &amp;gt; file.bam
&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;sort-bamfile&quot;&gt;Sort bamfile&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;
samtools sort file.bam sorted
&lt;/code&gt;
* outputs &lt;code&gt;sorted.bam&lt;/code&gt; file&lt;/p&gt;

&lt;h3 id=&quot;samblaster&quot;&gt;Samblaster&lt;/h3&gt;

&lt;p&gt;Extracts reads from &lt;code&gt;.sam&lt;/code&gt; alignment files&lt;/p&gt;

&lt;h4 id=&quot;get-discordant-reads&quot;&gt;Get discordant reads&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;
bowtie2 [options] | samblaster -e -d file.sam | samtools view -bS - &amp;gt; file.bam
&lt;/code&gt;
* Print output from bowtie to stderr to pipe directly into samblaster
* Saves having to search through sam or bam file later to extract discordant reads
* Pipe output directly into &lt;code&gt;samtools view&lt;/code&gt; to save as &lt;code&gt;.bam&lt;/code&gt; file (these won’t be the reads in the samblaster output)&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Highly Multiplexed Subcellular RNA Sequencing in Situ</title>
   <link href="//2014/03/04/highly-multiplexed-subcellular-rna-sequencing-in-situ/"/>
   <updated>2014-03-04T16:18:11+08:00</updated>
   <id>/2014/03/04/highly-multiplexed-subcellular-rna-sequencing-in-situ</id>
   <content type="html">&lt;p&gt;An improved method capable of sequencing RNA inside fixed cells (FISSEQ) was published in the early issue of &lt;i&gt;Science&lt;/i&gt; last week, titled &lt;i&gt;Highly Multiplexed Subcellular RNA Sequencing in Situ &lt;/i&gt;(1). It&#39;s an interesting method, and I&#39;m going to go over it briefly here.&lt;/p&gt;
&lt;p&gt;Firstly, the method is not new; it was developed in 2003, and this is stated in the first paragraph of the paper. The original 2003 method was conceptually the same, involving sequencing of DNA bases through the incorporation of fluorescent nucleotides. However, this was only capable of sequencing libraries fixed in gel on a glass slide, and produced a measly 8 bp of sequence (2). Needless to say, huge improvements in sequencing technologies have been made over the past ten years.&lt;/p&gt;
&lt;p&gt;The new method presented by Lee et al. works in the following way. Firstly, cells are fixed with formaldehyde and permeabilised, and an &lt;em&gt;in situ&lt;/em&gt; reverse transcription reaction performed with random hexamers and aminoallyl dUTP. cDNA is then circularised and amplified by rolling circle amplification. The authors report that the use of RNaseA improves the circularisation of cDNAs for rolling circle amplification (an RNaseH digest is also done after the RT to degrade RNA templates). dUTPs incorporated during the RT then allow crosslinking of the amplicons, through the addition of BS(PEG)9. This improves spacial stabilisation of the library &lt;em&gt;in situ&lt;/em&gt;&lt;em&gt;. &lt;/em&gt;Addition of BS(PEG)9 is also helpful in reducing non-specific probe binding. Next, a formamide wash removes unlinked cDNA amplicons, improving amplicon density, and fluorescent probes are hybridised to the adapter sequence. Finally, SOLiD sequencing-by-ligation can be performed, and the authors report read lengths of 27 bp (up from 8 bp in 2003), with a median error rate of 0.64%. Additionally, this method provides subcellular localisation data for reads. The end result is a bunch of cool flashing dots inside a cell:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;img src=&quot;/assets/s5_reduced1.gif&quot; width=&quot;250&quot; height=&quot;250&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Video S5 from Lee et al. (2014) (1).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The authors have compared the FISSEQ method to RNA-seq and gene expression arrays, and find a reasonably good correlation between some genes (those that were moderately expressed), though overall the correlation is poor:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;img src=&quot;/assets/s11c.png&quot; width=&quot;300&quot; height=&quot;220&quot; /&gt;&lt;br /&gt;
Figure S11C from Lee et al. (2014) (1).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;There also seems to be a partial depletion of reads involved in RNA and protein processing, and the authors suggest that this could be due to some cellular structures, such as the nucleolus, being inaccessible by the FISSEQ method. I would agree that this is the case, since there were far fewer rRNA reads detected than would be expected (only 42.7% in primary fibroblasts) and it&#39;s pretty clear from some of the figures and videos that there are amplicon-free regions:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;img src=&quot;/assets/s6b.png&quot; width=&quot;300&quot; height=&quot;296&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Figure S6B from Lee et al. (2014) (1).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Actually, it could be a good thing that some parts of the cell are not covered by FISSEQ, particularly the nucleolus, as this seems to provide a partial rRNA depletion.&lt;/p&gt;
&lt;p&gt;Overall, I think the method is an exciting one, and an impressive achievement with some promising applications in the future—the authors suggest &lt;em&gt;in situ &lt;/em&gt;cell type identification based on expression profiling, and high-throughput &lt;em&gt;in situ &lt;/em&gt;mutation detection. However, these applications are likely to be held back until longer read lengths can be achieved, and the protocol can simplified to reduce hands-on time. From the supplementary methods, I estimated just the SOLiD sequencing to take around two hours per cycle. Considering 27 cycles were done in these experiments, that would total 54 hours of hands-on lab time, for only 27 bp of sequence. Little mention is made of the amount of time needed for downstream informatics.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.sciencemag.org/content/early/2014/02/26/science.1250212&quot; target=&quot;_blank&quot;&gt;1. &lt;span style=&quot;line-height:1.5em;&quot;&gt;Lee, J. H., Daugharthy, E. R., Scheiman, J., Kalhor, R., Yang, J. L., Ferrante, T. C., et al. (2014). Highly Multiplexed Subcellular RNA Sequencing in Situ. Science. doi:10.1126/science.1250212&lt;/span&gt;&lt;/a&gt;&lt;a href=&quot;http://www.sciencemag.org/content/early/2014/02/26/science.1250212&quot; target=&quot;_blank&quot;&gt;&lt;br /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://arep.med.harvard.edu/pdf/Mitra03c.pdf&quot; target=&quot;_blank&quot;&gt;2. Mitra, R. D., Shendure, J., Olejnik, J., Edyta-Krzymanska-Olejnik, &amp;amp; Church, G. M. (2003). Fluorescent in situ sequencing on polymerase colonies. Analytical Biochemistry, 320(1), 55–65. doi:10.1016/S0003-2697(03)00291-4&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 

</feed>
